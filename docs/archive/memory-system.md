# Memory System Architecture

> **"Memory is not just storage—it's relationships. Understanding how ideas connect is how Talking Rock becomes truly personal."**

The hybrid vector-graph memory system extends the block architecture with semantic search and relational memory, enabling CAIRN to learn from interactions and recall relevant context.

---

## Overview

The memory system adds two capabilities to the existing block system:

1. **Semantic Search**: Find blocks by meaning, not just keywords
2. **Relationship Graph**: Navigate connections between ideas

```
┌─────────────────────────────────────────────────────────────────┐
│                    MEMORY GRAPH                                  │
│                                                                  │
│   [reasoning_chain] ──FOLLOWS_FROM──► [reasoning_chain]         │
│         │                                    │                   │
│    REFERENCES                           SUPPORTS                 │
│         ▼                                    ▼                   │
│   [knowledge_fact] ◄──SIMILAR_TO──► [knowledge_fact]            │
│         │                                                        │
│    RELATES_TO                                                    │
│         ▼                                                        │
│   [user_prompt] ──CAUSED_BY──► [consciousness_event]            │
└─────────────────────────────────────────────────────────────────┘
```

---

## Core Principles

### Local-First

All memory operations run locally:
- **Embeddings**: Generated by `all-MiniLM-L6-v2` (384 dimensions, runs on CPU)
- **Storage**: SQLite tables in `play.db`
- **No cloud APIs**: Privacy by architecture

### Block-Native

Memory extends the existing block system:
- Relationships connect blocks (new horizontal edges)
- Embeddings are stored per-block
- All memory is queryable via the Blocks API

### Learning from Feedback

The memory system integrates with RLHF:
- Positive feedback strengthens relationships
- Negative feedback weakens or creates corrections
- Patterns emerge from user behavior

---

## Architecture

### Three-Stage Retrieval Pipeline

```
User Message
     │
     ▼
┌────────────────────────────────────────────────────────────────┐
│                 STAGE 1: Semantic Search                        │
│                                                                 │
│   User text ──► Embed ──► Find similar blocks (cosine sim)     │
│                                                                 │
│   Returns: Top K blocks with similarity scores                  │
└────────────────────────────────────────────────────────────────┘
     │
     ▼
┌────────────────────────────────────────────────────────────────┐
│                 STAGE 2: Graph Expansion                        │
│                                                                 │
│   For each semantic match:                                      │
│   - Traverse outgoing relationships (depth 1-2)                │
│   - Prioritize: REFERENCES, FOLLOWS_FROM, SUPPORTS             │
│   - Collect related blocks with path info                      │
│                                                                 │
│   Returns: Related blocks with relationship chains              │
└────────────────────────────────────────────────────────────────┘
     │
     ▼
┌────────────────────────────────────────────────────────────────┐
│                 STAGE 3: Rank & Merge                          │
│                                                                 │
│   Combine semantic matches + graph expansions                   │
│   Apply scoring:                                                │
│   - Type weight (reasoning_chain > paragraph)                  │
│   - Source bonus (both > semantic > graph)                     │
│   - Recency (optional)                                         │
│                                                                 │
│   Returns: Ranked MemoryContext for CAIRN                      │
└────────────────────────────────────────────────────────────────┘
```

### Integration with CAIRN

Memory context is injected into CAIRN's prompt alongside other context sources:

```python
# In agent.py _build_full_context()
ctx = ConversationContext(
    persona_system=...,
    play_context=...,
    learned_context=...,
    memory_context=self._get_memory_context(user_text, act_id),  # NEW
    system_context=...,
    conversation_history=...,
)
```

The `memory_context` appears in the prompt as:

```markdown
## Relevant Memory

*Retrieved 5 relevant memories (3 semantic, 2 via relationships)*

### Memory #1 [████] (semantic)
*Type: reasoning_chain | Score: 0.85*

The user prefers concise responses without excessive explanation...

*Connected via: references → supports*
```

---

## Data Model

### Relationship Types

Relationships are typed edges between blocks:

| Category | Types | Description |
|----------|-------|-------------|
| **Logical** | `references`, `follows_from`, `contradicts`, `supports` | Reasoning connections |
| **Semantic** | `similar_to`, `related_to`, `elaborates` | Content similarity |
| **Causal** | `caused_by`, `causes` | Event chains |
| **Feedback** | `corrects`, `supersedes`, `derived_from` | Learning signals |
| **Temporal** | `preceded_by`, `responds_to` | Conversation flow |

### Relationship Source

Tracks how relationships were created:

| Source | Description |
|--------|-------------|
| `user` | Explicitly created by user |
| `cairn` | Created during CAIRN reasoning |
| `inferred` | Auto-extracted from content |
| `feedback` | Created from RLHF signals |
| `embedding` | Detected via semantic similarity |

### Database Schema

```sql
-- Typed relationships between blocks
CREATE TABLE block_relationships (
    id TEXT PRIMARY KEY,
    source_block_id TEXT NOT NULL REFERENCES blocks(id) ON DELETE CASCADE,
    target_block_id TEXT NOT NULL REFERENCES blocks(id) ON DELETE CASCADE,
    relationship_type TEXT NOT NULL,
    confidence REAL DEFAULT 1.0,        -- 0.0-1.0
    weight REAL DEFAULT 1.0,            -- For graph algorithms
    source TEXT NOT NULL,               -- 'user', 'cairn', 'inferred', etc.
    created_at TEXT NOT NULL,
    UNIQUE(source_block_id, target_block_id, relationship_type),
    CHECK(source_block_id != target_block_id)
);

-- Vector embeddings for semantic search
CREATE TABLE block_embeddings (
    block_id TEXT PRIMARY KEY REFERENCES blocks(id) ON DELETE CASCADE,
    embedding BLOB NOT NULL,            -- 384-dim float32 (1536 bytes)
    embedding_model TEXT DEFAULT 'all-MiniLM-L6-v2',
    content_hash TEXT NOT NULL,         -- For staleness detection
    created_at TEXT NOT NULL
);
```

---

## Components

### EmbeddingService

Singleton service for vector embeddings:

```python
from reos.memory import get_embedding_service

service = get_embedding_service()

# Generate embedding
embedding_bytes = service.embed("How do I configure the database?")

# Find similar
similar = service.find_similar(
    query_embedding,
    candidates,  # List of (block_id, embedding_bytes)
    threshold=0.5,
    top_k=10,
)
```

**Properties:**
- Model: `all-MiniLM-L6-v2` (384 dimensions)
- Lazy loading (model loads on first use)
- Thread-safe singleton
- Graceful degradation if model unavailable

### MemoryGraphStore

CRUD operations and graph traversal:

```python
from reos.memory import MemoryGraphStore, RelationshipType

store = MemoryGraphStore()

# Create relationship
rel_id = store.create_relationship(
    source_id="block-abc",
    target_id="block-xyz",
    rel_type=RelationshipType.REFERENCES,
    confidence=0.9,
)

# Traverse from a block
result = store.traverse(
    start_id="block-abc",
    max_depth=2,
    rel_types=[RelationshipType.REFERENCES, RelationshipType.SUPPORTS],
)
print(result.visited_blocks)
print(result.blocks_by_depth)
```

### MemoryRetriever

Three-stage retrieval for CAIRN context:

```python
from reos.memory import MemoryRetriever

retriever = MemoryRetriever()

context = retriever.retrieve(
    query="How should I handle authentication?",
    act_id="act-123",
    max_results=10,
    semantic_threshold=0.5,
    graph_depth=1,
)

# Format for CAIRN prompt
markdown = context.to_markdown()
```

### RelationshipExtractor

Automatic relationship extraction:

```python
from reos.memory.extractor import RelationshipExtractor

extractor = RelationshipExtractor()

# Extract from reasoning chain
relationships = extractor.extract_from_chain(
    chain_block_id="block-chain-1",
    chain_content="Because X, therefore Y...",
)

# Learn from feedback
changes = extractor.extract_from_feedback(
    chain_block_id="block-chain-1",
    rating=5,  # Strengthens relationships
)

# Auto-link similar blocks
count = extractor.auto_link_similar_blocks(
    threshold=0.8,
    max_links_per_block=3,
)
```

---

## RPC API

### Relationship Operations

| Endpoint | Description |
|----------|-------------|
| `memory/relationships/create` | Create a relationship between blocks |
| `memory/relationships/list` | List relationships for a block |
| `memory/relationships/update` | Update confidence/weight |
| `memory/relationships/delete` | Delete a relationship |

### Search & Retrieval

| Endpoint | Description |
|----------|-------------|
| `memory/search` | Semantic search with graph expansion |
| `memory/related` | Graph traversal from a block |
| `memory/path` | Find path between two blocks |

### Index Management

| Endpoint | Description |
|----------|-------------|
| `memory/index/block` | Index a single block |
| `memory/index/batch` | Index multiple blocks |
| `memory/index/remove` | Remove block from index |

### Learning

| Endpoint | Description |
|----------|-------------|
| `memory/extract` | Extract relationships from content |
| `memory/learn` | Learn from RLHF feedback |
| `memory/auto_link` | Auto-create similarity links |

### Statistics

| Endpoint | Description |
|----------|-------------|
| `memory/stats` | Get memory system statistics |

---

## Usage Examples

### Creating Relationships via RPC

```json
// Request
{
  "method": "memory/relationships/create",
  "params": {
    "source_id": "block-abc123",
    "target_id": "block-xyz789",
    "rel_type": "references",
    "confidence": 0.9,
    "source": "user"
  }
}

// Response
{
  "relationship": {
    "id": "rel-abc123def456",
    "source_block_id": "block-abc123",
    "target_block_id": "block-xyz789",
    "relationship_type": "references",
    "confidence": 0.9,
    "weight": 1.0,
    "source": "user",
    "created_at": "2026-01-26T10:30:00Z"
  }
}
```

### Searching Memory

```json
// Request
{
  "method": "memory/search",
  "params": {
    "query": "How do I configure authentication?",
    "act_id": "act-main",
    "max_results": 10,
    "include_graph": true
  }
}

// Response
{
  "query": "How do I configure authentication?",
  "matches": [
    {
      "block_id": "block-auth-setup",
      "block_type": "knowledge_fact",
      "content": "Authentication uses JWT tokens stored in...",
      "score": 0.82,
      "source": "semantic",
      "relationship_chain": []
    },
    {
      "block_id": "block-security-notes",
      "block_type": "paragraph",
      "content": "Security considerations for auth...",
      "score": 0.65,
      "source": "graph",
      "relationship_chain": ["references"]
    }
  ],
  "stats": {
    "total_semantic": 5,
    "total_graph": 3,
    "returned": 8
  }
}
```

### Graph Traversal

```json
// Request
{
  "method": "memory/related",
  "params": {
    "block_id": "block-abc123",
    "depth": 2,
    "rel_types": ["references", "follows_from"],
    "direction": "both"
  }
}

// Response
{
  "start_block_id": "block-abc123",
  "visited_blocks": ["block-abc123", "block-def456", "block-ghi789"],
  "blocks_by_depth": {
    "0": ["block-abc123"],
    "1": ["block-def456"],
    "2": ["block-ghi789"]
  },
  "edges": [
    {
      "id": "rel-1",
      "source_block_id": "block-abc123",
      "target_block_id": "block-def456",
      "relationship_type": "references",
      "confidence": 0.9
    }
  ]
}
```

---

## Learning Flow

### Positive Feedback (Rating 4-5)

```
User rates reasoning chain 5/5
           │
           ▼
┌──────────────────────────────────────┐
│ For each outgoing relationship:      │
│ - Increase confidence by 0.1         │
│ - Cap at 1.0                         │
└──────────────────────────────────────┘
           │
           ▼
Relationships strengthened for future retrieval
```

### Negative Feedback (Rating 1-2)

```
User rates reasoning chain 2/5
User provides corrected response
           │
           ▼
┌──────────────────────────────────────┐
│ 1. Create CORRECTS relationship:     │
│    corrected_block → original_chain  │
│                                      │
│ 2. For each outgoing relationship:   │
│    - Decrease confidence by 0.2      │
│    - Floor at 0.1                    │
└──────────────────────────────────────┘
           │
           ▼
System learns from correction
```

---

## Performance Targets

| Operation | Target | Notes |
|-----------|--------|-------|
| Embedding generation | < 10ms | Per block, after model loaded |
| Model cold start | < 2s | First embedding, loads model |
| Semantic search (10K blocks) | < 50ms | In-memory comparison |
| Graph traversal (depth=2) | < 10ms | SQLite indexed queries |
| Full retrieval pipeline | < 100ms | All three stages |

---

## Configuration

Memory retrieval can be disabled via the context overlay:

```python
# In UI settings
disabled_sources = ["memory"]  # Disables memory context injection
```

Embedding service gracefully degrades if sentence-transformers is not installed:
- Semantic search returns empty results
- Graph operations still work
- Relationships can still be created manually

---

## Document Integration

The memory system indexes document chunks for RAG (Retrieval-Augmented Generation). When users insert documents via the `/document` slash command, the system:

1. **Extracts text** from PDF, DOCX, TXT, MD, CSV, XLSX files
2. **Chunks text** into ~500 token segments with overlap
3. **Creates blocks** of type `document_chunk` for each segment
4. **Indexes blocks** via `memory/index/batch` for semantic search

### How Documents Appear in Memory

Document chunks are treated like any other block:

```
User: "What Python experience do I have?"
        │
        ▼
┌────────────────────────────────────────────────────────┐
│ Memory Retrieval                                        │
│                                                         │
│ Semantic search returns:                                │
│ - document_chunk (resume.pdf, page 1): 0.85 score      │
│ - paragraph (career notes): 0.72 score                 │
│ - document_chunk (resume.pdf, page 2): 0.68 score      │
└────────────────────────────────────────────────────────┘
```

### Chunk Properties

Each `document_chunk` block has properties that preserve source context:

| Property | Description |
|----------|-------------|
| `source_document_id` | Links back to parent document |
| `chunk_index` | Position in document (0-based) |
| `page_number` | Source page for PDFs |
| `section_title` | Detected section header |
| `total_chunks` | Total chunks in document |

This metadata appears in memory context, helping CAIRN cite sources:

```markdown
### Memory #3 [████] (semantic)
*Type: document_chunk | Score: 0.85 | Source: resume.pdf (page 1)*

Senior Python developer with 5 years experience building Django applications...
```

See [Documents](./documents.md) for complete documentation on document ingestion.

---

## Future Extensions

- **Visualization**: Graph view of block relationships in UI
- **Pruning**: Decay old/unused relationships over time
- **Clustering**: Auto-group similar knowledge into topics
- **Export**: Generate training data for fine-tuning
- **sqlite-vec**: Native vector extension for larger scale

---

## Related Documentation

- [Blocks API](./blocks-api.md) — Block system that memory extends
- [Documents](./documents.md) — Document ingestion and RAG
- [RLHF Learning](./rlhf-learning.md) — Feedback system that feeds memory
- [CAIRN Architecture](./cairn_architecture.md) — Agent that uses memory context
- [Foundation](./FOUNDATION.md) — Core principles (local-first, privacy)
